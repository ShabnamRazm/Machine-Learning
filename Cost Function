# will implement and explore the cost function for linear regression with one variable.
#Tools:
# NumPy, a popular library for scientific computing
# Matplotlib, a popular library for plotting data
# local plotting routines in the lab_utils_uni.py file in the local directory

import numpy as np
#? %matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl
plt.style.use(''C:\\Users\\Admin\\Desktop\\APPLY\\Python\\New folder (2)\\New folder\\deeplearning.mplstyle')

# You would like a model which can predict housing prices given the size of the house.
# Let's use the same two data points as before the previous lab- a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.

x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)
y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)

#The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.

#ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)) is our prediction for example  ğ‘– using parameters  ğ‘¤,ğ‘
#(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))^2is the squared difference between the target value and the prediction.
#These differences are summed over all the  ğ‘š examples and divided by 2m to produce the cost,  ğ½(ğ‘¤,ğ‘)
#Note, in lecture summation ranges are typically from 1 to m, while code will be from 0 to m-1.

#The code below calculates cost by looping over each example. In each loop:
#f_wb, a prediction is calculated
#the difference between the target and the prediction is calculated and squared.
#this is added to the total cost.

def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    m = x.shape[0] 
    
    cost_sum = 0 
    for i in range(m): 
        f_wb = w * x[i] + b   
        cost = (f_wb - y[i]) ** 2  
        cost_sum = cost_sum + cost  
    total_cost = (1 / (2 * m)) * cost_sum  

    return total_cost
#Your goal is to find a model  ğ‘“ğ‘¤,ğ‘(ğ‘¥)=ğ‘¤ğ‘¥+ğ‘, with parameters  ğ‘¤,ğ‘, which will accurately predict house values given an input  ğ‘¥
#The cost is a measure of how accurate the model is on the training data.
# if  ğ‘¤ and  ğ‘ can be selected such that the predictions  ğ‘“ğ‘¤,ğ‘(ğ‘¥) match the target data  ğ‘¦ , the  (ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))^2 term will be zero and the cost minimized
#In the previous lab, you determined that  ğ‘=100 provided an optimal solution so let's set  ğ‘ to 100 and focus on  ğ‘¤

plt_intuition(x_train,y_train)

#LARGER Dataset:
x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])
y_train = np.array([250, 300, 480,  430,   630, 730,])

plt.close('all') 
fig, ax, dyn_items = plt_stationary(x_train, y_train)
updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)

soup_bowl()
