#automate the process of optimizing  ğ‘¤ and  ğ‘ using gradient descent.
#Make use of: 
#NumPy, a popular library for scientific computing
#Matplotlib, a popular library for plotting data
#plotting routines in the lab_utils.py file in the local directory

import math, copy
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients

#Let's use the same two data points as before - a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.
#Load our data set
x_train=np.array([1.0, 2.0])   #features
y_trian=np.array([300.0,500.0])    #Target values

#Function to calculate the cost
def compute_cost(x, y, w, b):
  m=x.shape[0]
  cost=0
  for i in range(m):
    f_wb=w*x[i]+b
    cost=cost+(f_wb-y[i])^2
    total_cost=(1/(2*m))*cost
  return total_cost

#So far in this course, you have developed a linear model that predicts  ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)):    ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))=ğ‘¤ğ‘¥(ğ‘–)+ğ‘
#In linear regression, you utilize input training data to fit the parameters ğ‘¤,ğ‘ by minimizing a measure of the error between our predictions ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)) and the actual data ğ‘¦(ğ‘–)
#The measure is called the ğ‘ğ‘œğ‘ ğ‘¡, ğ½(ğ‘¤,ğ‘). In training you measure the cost over all of our training samples  ğ‘¥(ğ‘–),ğ‘¦(ğ‘–)
#ğ½(ğ‘¤,ğ‘)=1/2ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))^2

#gradient descent: Repeat until convergence:  w= ğ‘¤âˆ’ğ›¼*(âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘¤)  b=ğ‘âˆ’ğ›¼*(âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘)
#where, parameters  ğ‘¤,  b are updated simultaneously. The gradient is defined as:  âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘¤=(1/m)*(âˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))*ğ‘¥(ğ‘–))  âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚b=(1/m)*(âˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–)))
#Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.
#You will implement gradient descent algorithm for one feature. You will need three functions:compute_gradient/compute_cost/gradient_descent:utilizing compute_gradient and compute_cost
#Conventions:
#  The naming of python variables containing partial derivatives follows this pattern, âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘ will be dj_db.
#  w.r.t is With Respect To, as in partial derivative of  ğ½(ğ‘¤ğ‘) With Respect To  ğ‘

def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
   # Number of training examples
    m = x.shape[0]    
    dj_dw = 0
    dj_db = 0
        for i in range(m):  
            f_wb = w * x[i] + b 
            dj_dw_i = (f_wb - y[i]) * x[i] 
            dj_db_i = f_wb - y[i] 
            dj_db += dj_db_i
            dj_dw += dj_dw_i 
            dj_dw = dj_dw / m 
            dj_db = dj_db / m 
        
    return dj_dw, dj_db

#The lectures described how gradient descent utilizes the partial derivative of the cost with respect to a parameter at a point to update that parameter.
#Let's use our compute_gradient function to find and plot some partial derivatives of our cost function relative to one of the parameters,  ğ‘¤0

plt_gradients(x_train,y_train, compute_cost, compute_gradient)
plt.show()

#you will utilize this function to find optimal values of  ğ‘¤ and  ğ‘ on the training data:
