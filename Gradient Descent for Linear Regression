#automate the process of optimizing  𝑤 and  𝑏 using gradient descent.
#Make use of: 
#NumPy, a popular library for scientific computing
#Matplotlib, a popular library for plotting data
#plotting routines in the lab_utils.py file in the local directory

import math, copy
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients

#Let's use the same two data points as before - a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.
#Load our data set
x_train=np.array([1.0, 2.0])   #features
y_trian=np.array([300.0,500.0])    #Target values

#Function to calculate the cost
def compute_cost(x, y, w, b):
  m=x.shape[0]
  cost=0
  for i in range(m):
    f_wb=w*x[i]+b
    cost=cost+(f_wb-y[i])^2
    total_cost=(1/(2*m))*cost
  return total_cost

#So far in this course, you have developed a linear model that predicts  𝑓𝑤,𝑏(𝑥(𝑖)):    𝑓𝑤,𝑏(𝑥(𝑖))=𝑤𝑥(𝑖)+𝑏
#In linear regression, you utilize input training data to fit the parameters 𝑤,𝑏 by minimizing a measure of the error between our predictions 𝑓𝑤,𝑏(𝑥(𝑖)) and the actual data 𝑦(𝑖)
#The measure is called the 𝑐𝑜𝑠𝑡, 𝐽(𝑤,𝑏). In training you measure the cost over all of our training samples  𝑥(𝑖),𝑦(𝑖)
#𝐽(𝑤,𝑏)=1/2𝑚∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))^2

#gradient descent: Repeat until convergence:  w= 𝑤−𝛼*(∂𝐽(𝑤,𝑏)/∂𝑤)  b=𝑏−𝛼*(∂𝐽(𝑤,𝑏)/∂𝑏)
#where, parameters  𝑤,  b are updated simultaneously. The gradient is defined as:  ∂𝐽(𝑤,𝑏)/∂𝑤=(1/m)*(∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))*𝑥(𝑖))  ∂𝐽(𝑤,𝑏)/∂b=(1/m)*(∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖)))
#Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.
#You will implement gradient descent algorithm for one feature. You will need three functions:compute_gradient/compute_cost/gradient_descent:utilizing compute_gradient and compute_cost
#Conventions:
#  The naming of python variables containing partial derivatives follows this pattern, ∂𝐽(𝑤,𝑏)/∂𝑏 will be dj_db.
#  w.r.t is With Respect To, as in partial derivative of  𝐽(𝑤𝑏) With Respect To  𝑏

def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
   # Number of training examples
    m = x.shape[0]    
    dj_dw = 0
    dj_db = 0
        for i in range(m):  
            f_wb = w * x[i] + b 
            dj_dw_i = (f_wb - y[i]) * x[i] 
            dj_db_i = f_wb - y[i] 
            dj_db += dj_db_i
            dj_dw += dj_dw_i 
            dj_dw = dj_dw / m 
            dj_db = dj_db / m 
        
    return dj_dw, dj_db

#The lectures described how gradient descent utilizes the partial derivative of the cost with respect to a parameter at a point to update that parameter.
#Let's use our compute_gradient function to find and plot some partial derivatives of our cost function relative to one of the parameters,  𝑤0

plt_gradients(x_train,y_train, compute_cost, compute_gradient)
plt.show()

#you will utilize this function to find optimal values of  𝑤 and  𝑏 on the training data:
